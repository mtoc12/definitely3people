{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import copy as cp\n",
    "import sys\n",
    "\n",
    "\n",
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_list(path):\n",
    "    \"\"\"\n",
    "    Loads a list of the words from the file at path <path>, removing all\n",
    "    non-alpha-numeric characters from the file.\n",
    "    \"\"\"\n",
    "    with open(path) as handle:\n",
    "        # Load a list of whitespace-delimited words from the specified file\n",
    "        raw_text = handle.read().strip().split()\n",
    "        # Strip non-alphanumeric characters from each word\n",
    "        alphanumeric_words = map(\n",
    "            lambda word: \"\".join(char for char in word if char.isalnum()), raw_text\n",
    "        )\n",
    "        # Filter out words that are now empty (e.g. strings that only contained non-alphanumeric chars)\n",
    "        alphanumeric_words = filter(lambda word: len(word) > 0, alphanumeric_words)\n",
    "        # Convert each word to lowercase and return the result\n",
    "        return list(map(lambda word: word.lower(), alphanumeric_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_onehot_word_dict(word_list):\n",
    "    \"\"\"\n",
    "    Takes a list of the words in a text file, returning a dictionary mapping\n",
    "    words to their index in a one-hot-encoded representation of the words.\n",
    "    \"\"\"\n",
    "\n",
    "    word_to_index = []\n",
    "    i = 0\n",
    "    for word in word_list:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index.append(word)\n",
    "            i += 1\n",
    "\n",
    "    vect_length = len(word_to_index)\n",
    "    one_hot_dict = {}\n",
    "\n",
    "    for one_hot, word in enumerate(word_to_index):\n",
    "        vector = np.zeros(vect_length)\n",
    "        vector[one_hot] = 1\n",
    "        one_hot_dict[word] = vector\n",
    "\n",
    "    return one_hot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = load_word_list(\"shakespeare.txt\")\n",
    "\n",
    "for i in range(1, 155):\n",
    "    word_list.remove(\"{}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_onehot_char_dict(word_list, padding = 2):\n",
    "    \"\"\"\n",
    "    Takes a single word and converts it into a one-hot matrix.\n",
    "       Every row in the matrix corresponds to a character position.\n",
    "       Every column corresponds to the character in alphabetical order.\n",
    "       \n",
    "       The word list is used to find the matrix size. \n",
    "       We use two zero-rows as padding at each end.\n",
    "       \n",
    "       Output is a dictionnary corresponding to the one-hot matrix\n",
    "       of each word. \n",
    "    \"\"\"\n",
    "    # Create ordered alphabetic list\n",
    "    alphabet = []\n",
    "    for i in range(0, 26):\n",
    "        alphabet.append(chr(ord(\"a\") + i))\n",
    "\n",
    "    # Find word of largest character length in list\n",
    "    largest = 0\n",
    "    for i in word_list:\n",
    "        length = len(i)\n",
    "        if length > largest:\n",
    "            largest = length\n",
    "\n",
    "    # Create zero matrix + padding\n",
    "    matrix_origin = np.zeros((largest + (padding * 2), len(alphabet)))\n",
    "\n",
    "    # Initialize dictionary\n",
    "    one_hot_dict = {}\n",
    "\n",
    "    # Grab words in the list in order\n",
    "    for word in word_list:\n",
    "        matrix_copy = cp.copy(matrix_origin)\n",
    "\n",
    "        # Go through each character of that word\n",
    "        for char in range(len(word)):\n",
    "\n",
    "            # Compare with index in alphabet\n",
    "            for index in range(len(alphabet)):\n",
    "\n",
    "                # Compare letters\n",
    "                if alphabet[index] == word[char]:\n",
    "\n",
    "                    # Start writing two rows in (padding)\n",
    "                    matrix_copy[padding + char, index] = 1\n",
    "\n",
    "        one_hot_dict[word] = matrix_copy\n",
    "\n",
    "    return one_hot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_decoder(word):\n",
    "    \"\"\"\n",
    "    Takes a one-hot encoded word as an argument.\n",
    "    Output:\n",
    "    String\n",
    "    \"\"\"\n",
    "\n",
    "    # First remove the padding\n",
    "    word = word[~np.all(word == 0, axis=1)]\n",
    "\n",
    "    # Create ordered alphabetic list\n",
    "    alphabet = []\n",
    "    for i in range(0, 26):\n",
    "        alphabet.append(chr(ord(\"a\") + i))\n",
    "\n",
    "    output = \"\"\n",
    "\n",
    "    for char in word:\n",
    "        for pos in range(len(char)):\n",
    "            if char[pos] == 1:\n",
    "                output = output + alphabet[pos]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_dict = generate_onehot_char_dict(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from HW5-3\n",
    "\n",
    "def generate_traindata(word_list, window_size=4):\n",
    "    \"\"\"\n",
    "    Generates training data for Skipgram model.\n",
    "\n",
    "    Arguments:\n",
    "        word_list:     Sequential list of words (strings).\n",
    "        word_to_index: Dictionary mapping words to their corresponding index\n",
    "                       in a one-hot-encoded representation of our corpus.\n",
    "\n",
    "        window_size:   Size of Skipgram window.\n",
    "                       (use the default value when running your code).\n",
    "\n",
    "    Returns:\n",
    "        (trainX, trainY):     A pair of matrices (trainX, trainY) containing training \n",
    "                              points (one-hot-encoded vectors representing individual words) and \n",
    "                              their corresponding labels (also one-hot-encoded vectors representing words).\n",
    "\n",
    "                              For each index i, trainX[i] should correspond to a word in\n",
    "                              <word_list>, and trainY[i] should correspond to one of the words within\n",
    "                              a window of size <window_size> of trainX[i].\n",
    "    \"\"\"\n",
    "    one_hot = generate_onehot_char_dict(word_list, padding = 0)\n",
    "    \n",
    "    trainX = []\n",
    "    trainY = []\n",
    "\n",
    "    for index in range(len(word_list)):\n",
    "        \n",
    " \n",
    "        word = word_list[index]\n",
    "        matrix = one_hot[word]\n",
    "        \n",
    "        for i in [x for x in range(-window_size, window_size + 1) if x != 0]:\n",
    "\n",
    "            if (index + i) >= 0 and (index + i) < len(word_list):\n",
    "\n",
    "                onehot_x = matrix\n",
    "                onehot_y = one_hot[word_list[index + i]]\n",
    "\n",
    "                trainX.append(onehot_x)\n",
    "                trainY.append(onehot_y)\n",
    "\n",
    "    return (np.array(trainX), np.array(trainY)), one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding(word_list, num_latent_factors=10):\n",
    "    \"\"\" \n",
    "    Train a word2vec type embedding of the training data.\n",
    "        Data innput format: List\n",
    "        \n",
    "    Uses a dense, two layer shallow neural net to embed.  \n",
    "    Arguments:\n",
    "        word_list, constitutes training data\n",
    "        num_latent_factors, number of latent factors\n",
    "        \n",
    "    \"\"\"\n",
    "    train_data = generate_traindata(word_list)\n",
    "\n",
    "    trainX, trainY = train_data[0][0], train_data[0][1]\n",
    "\n",
    "    one_hot_dict = train_data[1]\n",
    "\n",
    "    input_dimension = 26\n",
    "    input_shape = (len(trainX[0]), 26)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dimension, input_shape=input_shape, activation=\"relu\"))\n",
    "    model.add(Dense(num_latent_factors, activation=\"relu\"))\n",
    "    model.add(Dense(input_dimension))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    model.fit(trainX, trainY, batch_size=30, epochs=80)\n",
    "\n",
    "    # Get model weights up to the second layer\n",
    "    weights_layer_1 = model.get_weights()[0]\n",
    "    weights_layer_2 = model.get_weights()[1]\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    for weight in model.get_weights():\n",
    "        print(\"Weight Shape:\", weight.shape)\n",
    "\n",
    "    # Sanity check, predict words from first 30 words in the training set\n",
    "    model_output = model.predict(trainX[:30])\n",
    "    input_word_list = []\n",
    "    output_word_list = []\n",
    "    \n",
    "    for i in model_output:\n",
    "        for j in range(len(i)):\n",
    "            index = np.argmax(i[j])\n",
    "            i[j] = np.zeros(len(i[j]))\n",
    "            i[j][index] = 1\n",
    "            \n",
    "        output_word_list.append(one_hot_decoder(i))\n",
    "    \n",
    "    for word in trainX[:30]:\n",
    "        input_word_list.append(one_hot_decoder(word))\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    print(\"Most likely next word: \")\n",
    "    \n",
    "    for k in range(len(input_word_list)):\n",
    "        print(\"\\n\")\n",
    "        print(input_word_list[k], \", \", output_word_list[k])\n",
    "\n",
    "    return model_output #weights_layer_1, weights_layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 17, 26)            702       \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 17, 10)            270       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 17, 26)            286       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 17, 26)            0         \n",
      "=================================================================\n",
      "Total params: 1,258\n",
      "Trainable params: 1,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "Epoch 1/80\n",
      "140636/140636 [==============================] - 10s 73us/step - loss: 0.6997 - accuracy: 0.0339\n",
      "Epoch 2/80\n",
      "140636/140636 [==============================] - 11s 77us/step - loss: 0.6938 - accuracy: 0.0347\n",
      "Epoch 3/80\n",
      "140636/140636 [==============================] - 11s 76us/step - loss: 0.6937 - accuracy: 0.0347\n",
      "Epoch 4/80\n",
      "140636/140636 [==============================] - 10s 72us/step - loss: 0.6937 - accuracy: 0.0347\n",
      "Epoch 5/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6936 - accuracy: 0.0347\n",
      "Epoch 6/80\n",
      "140636/140636 [==============================] - 10s 70us/step - loss: 0.6935 - accuracy: 0.0347\n",
      "Epoch 7/80\n",
      "140636/140636 [==============================] - 10s 72us/step - loss: 0.6935 - accuracy: 0.0347\n",
      "Epoch 8/80\n",
      "140636/140636 [==============================] - 11s 79us/step - loss: 0.6935 - accuracy: 0.0347\n",
      "Epoch 9/80\n",
      "140636/140636 [==============================] - 10s 72us/step - loss: 0.6934 - accuracy: 0.0347\n",
      "Epoch 10/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6934 - accuracy: 0.0347\n",
      "Epoch 11/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6934 - accuracy: 0.0347\n",
      "Epoch 12/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6934 - accuracy: 0.0347\n",
      "Epoch 13/80\n",
      "140636/140636 [==============================] - 10s 70us/step - loss: 0.6934 - accuracy: 0.0348\n",
      "Epoch 14/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6934 - accuracy: 0.0347\n",
      "Epoch 15/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6934 - accuracy: 0.0347\n",
      "Epoch 16/80\n",
      "140636/140636 [==============================] - 10s 70us/step - loss: 0.6934 - accuracy: 0.0347\n",
      "Epoch 17/80\n",
      "140636/140636 [==============================] - 10s 70us/step - loss: 0.6934 - accuracy: 0.0347\n",
      "Epoch 18/80\n",
      "140636/140636 [==============================] - 10s 70us/step - loss: 0.6934 - accuracy: 0.0347\n",
      "Epoch 19/80\n",
      "140636/140636 [==============================] - 10s 70us/step - loss: 0.6934 - accuracy: 0.0347\n",
      "Epoch 20/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6934 - accuracy: 0.0347\n",
      "Epoch 21/80\n",
      "140636/140636 [==============================] - 10s 70us/step - loss: 0.6933 - accuracy: 0.0347\n",
      "Epoch 22/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6933 - accuracy: 0.0347\n",
      "Epoch 23/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6933 - accuracy: 0.0347\n",
      "Epoch 24/80\n",
      "140636/140636 [==============================] - 10s 70us/step - loss: 0.6933 - accuracy: 0.0347\n",
      "Epoch 25/80\n",
      "140636/140636 [==============================] - 10s 69us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 26/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6933 - accuracy: 0.0347\n",
      "Epoch 27/80\n",
      "140636/140636 [==============================] - 10s 72us/step - loss: 0.6933 - accuracy: 0.0347\n",
      "Epoch 28/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6933 - accuracy: 0.0347\n",
      "Epoch 29/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 30/80\n",
      "140636/140636 [==============================] - 10s 69us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 31/80\n",
      "140636/140636 [==============================] - 10s 70us/step - loss: 0.6933 - accuracy: 0.0347\n",
      "Epoch 32/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 33/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 34/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6933 - accuracy: 0.0347\n",
      "Epoch 35/80\n",
      "140636/140636 [==============================] - 10s 72us/step - loss: 0.6933 - accuracy: 0.0347\n",
      "Epoch 36/80\n",
      "140636/140636 [==============================] - 10s 72us/step - loss: 0.6933 - accuracy: 0.0347\n",
      "Epoch 37/80\n",
      "140636/140636 [==============================] - 10s 72us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 38/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 39/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 40/80\n",
      "140636/140636 [==============================] - 10s 72us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 41/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 42/80\n",
      "140636/140636 [==============================] - 10s 72us/step - loss: 0.6933 - accuracy: 0.0347\n",
      "Epoch 43/80\n",
      "140636/140636 [==============================] - 10s 72us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 44/80\n",
      "140636/140636 [==============================] - 10s 70us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 45/80\n",
      "140636/140636 [==============================] - 10s 72us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 46/80\n",
      "140636/140636 [==============================] - 10s 72us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 47/80\n",
      "140636/140636 [==============================] - 10s 73us/step - loss: 0.6932 - accuracy: 0.0348\n",
      "Epoch 48/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 49/80\n",
      "140636/140636 [==============================] - 10s 72us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 50/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 51/80\n",
      "140636/140636 [==============================] - 10s 72us/step - loss: 0.6932 - accuracy: 0.0347\n",
      "Epoch 52/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6932 - accuracy: 0.0348\n",
      "Epoch 53/80\n",
      "140636/140636 [==============================] - 10s 72us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 54/80\n",
      "140636/140636 [==============================] - 10s 70us/step - loss: 0.6933 - accuracy: 0.0348\n",
      "Epoch 55/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6932 - accuracy: 0.0348\n",
      "Epoch 56/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6932 - accuracy: 0.0349\n",
      "Epoch 57/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6932 - accuracy: 0.0348\n",
      "Epoch 58/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6933 - accuracy: 0.0347\n",
      "Epoch 59/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6932 - accuracy: 0.0348\n",
      "Epoch 60/80\n",
      "140636/140636 [==============================] - 10s 70us/step - loss: 0.6932 - accuracy: 0.0348\n",
      "Epoch 61/80\n",
      "140636/140636 [==============================] - 10s 71us/step - loss: 0.6932 - accuracy: 0.0348\n",
      "Epoch 62/80\n",
      "140636/140636 [==============================] - 10s 69us/step - loss: 0.6932 - accuracy: 0.0348\n",
      "Epoch 63/80\n",
      "140636/140636 [==============================] - 10s 70us/step - loss: 0.6932 - accuracy: 0.0348\n",
      "Epoch 64/80\n",
      "140636/140636 [==============================] - 10s 72us/step - loss: 0.6932 - accuracy: 0.0348\n",
      "Epoch 65/80\n",
      "140636/140636 [==============================] - 10s 72us/step - loss: 0.6932 - accuracy: 0.0347\n",
      "Epoch 66/80\n",
      "140636/140636 [==============================] - 10s 70us/step - loss: 0.6932 - accuracy: 0.0348\n",
      "Epoch 67/80\n",
      "140636/140636 [==============================] - 10s 73us/step - loss: 0.6932 - accuracy: 0.0348\n",
      "Epoch 68/80\n",
      "140636/140636 [==============================] - 12s 85us/step - loss: 0.6932 - accuracy: 0.0348\n",
      "Epoch 69/80\n",
      "140636/140636 [==============================] - 12s 82us/step - loss: 0.6932 - accuracy: 0.0348\n",
      "Epoch 70/80\n",
      " 16320/140636 [==>...........................] - ETA: 9s - loss: 0.6907 - accuracy: 0.0345"
     ]
    }
   ],
   "source": [
    "test = train_embedding(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
