{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import copy as cp\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "%load_ext blackcellmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word_list(path):\n",
    "    \"\"\"\n",
    "    Loads a list of the words from the file at path <path>, removing all\n",
    "    non-alpha-numeric characters from the file.\n",
    "    \"\"\"\n",
    "    with open(path) as handle:\n",
    "        # Load a list of whitespace-delimited words from the specified file\n",
    "        raw_text = handle.read().strip().split()\n",
    "        # Strip non-alphanumeric characters from each word\n",
    "        alphanumeric_words = map(\n",
    "            lambda word: \"\".join(char for char in word if char.isalnum()), raw_text\n",
    "        )\n",
    "        # Filter out words that are now empty (e.g. strings that only contained non-alphanumeric chars)\n",
    "        alphanumeric_words = filter(lambda word: len(word) > 0, alphanumeric_words)\n",
    "        # Convert each word to lowercase and return the result\n",
    "        return list(map(lambda word: word.lower(), alphanumeric_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_onehot_word_dict(word_list):\n",
    "    \"\"\"\n",
    "    Takes a list of the words in a text file, returning a dictionary mapping\n",
    "    words to their index in a one-hot-encoded representation of the words.\n",
    "    \"\"\"\n",
    "\n",
    "    word_to_index = []\n",
    "    i = 0\n",
    "    for word in word_list:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index.append(word)\n",
    "            i += 1\n",
    "\n",
    "    vect_length = len(word_to_index)\n",
    "    one_hot_dict = {}\n",
    "\n",
    "    for one_hot, word in enumerate(word_to_index):\n",
    "        vector = np.zeros(vect_length)\n",
    "        vector[one_hot] = 1\n",
    "        one_hot_dict[word] = vector\n",
    "\n",
    "    return one_hot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = load_word_list(\"shakespeare.txt\")\n",
    "\n",
    "for i in range(1, 155):\n",
    "    word_list.remove(\"{}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_onehot_char_dict(word_list, padding = 2):\n",
    "    \"\"\"\n",
    "    Takes a single word and converts it into a one-hot matrix.\n",
    "       Every row in the matrix corresponds to a character position.\n",
    "       Every column corresponds to the character in alphabetical order.\n",
    "       \n",
    "       The word list is used to find the matrix size. \n",
    "       We use two zero-rows as padding at each end.\n",
    "       \n",
    "       Output is a dictionnary corresponding to the one-hot matrix\n",
    "       of each word. \n",
    "    \"\"\"\n",
    "    # Create ordered alphabetic list\n",
    "    alphabet = []\n",
    "    for i in range(0, 26):\n",
    "        alphabet.append(chr(ord(\"a\") + i))\n",
    "\n",
    "    # Find word of largest character length in list\n",
    "    largest = 0\n",
    "    for i in word_list:\n",
    "        length = len(i)\n",
    "        if length > largest:\n",
    "            largest = length\n",
    "\n",
    "    # Create zero matrix + padding\n",
    "    matrix_origin = np.zeros((largest + (padding * 2), len(alphabet)))\n",
    "\n",
    "    # Initialize dictionary\n",
    "    one_hot_dict = {}\n",
    "\n",
    "    # Grab words in the list in order\n",
    "    for word in word_list:\n",
    "        matrix_copy = cp.copy(matrix_origin)\n",
    "\n",
    "        # Go through each character of that word\n",
    "        for char in range(len(word)):\n",
    "\n",
    "            # Compare with index in alphabet\n",
    "            for index in range(len(alphabet)):\n",
    "\n",
    "                # Compare letters\n",
    "                if alphabet[index] == word[char]:\n",
    "\n",
    "                    # Start writing two rows in (padding)\n",
    "                    matrix_copy[padding + char, index] = 1\n",
    "\n",
    "        one_hot_dict[word] = matrix_copy\n",
    "\n",
    "    return one_hot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_decoder(word, word_list, char_based = False):\n",
    "    \"\"\"\n",
    "    Takes a one-hot encoded word as an argument.\n",
    "    Output:\n",
    "    String\n",
    "    \"\"\"\n",
    "    \n",
    "    if char_based:\n",
    "        # First remove the padding\n",
    "        word = word[~np.all(word == 0, axis=1)]\n",
    "\n",
    "        # Create ordered alphabetic list\n",
    "        alphabet = []\n",
    "        for i in range(0, 26):\n",
    "            alphabet.append(chr(ord(\"a\") + i))\n",
    "\n",
    "        output = \"\"\n",
    "\n",
    "        for char in word:\n",
    "            for pos in range(len(char)):\n",
    "                if char[pos] == 1:\n",
    "                    output = output + alphabet[pos]\n",
    "                    \n",
    "    else:\n",
    "        word_to_index = []\n",
    "        \n",
    "        for words in word_list:\n",
    "            if words not in word_to_index:\n",
    "                word_to_index.append(words)\n",
    "        \n",
    "        for i in range(len(word)):\n",
    "            if word[i] == 1:\n",
    "                output = word_to_index[i]\n",
    "\n",
    "                return output\n",
    "            \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_dict = generate_onehot_char_dict(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from HW5-3\n",
    "\n",
    "def generate_traindata(word_list, window_size=4, char_based = False):\n",
    "    \"\"\"\n",
    "    Generates training data for Skipgram model.\n",
    "\n",
    "    Arguments:\n",
    "        word_list:     Sequential list of words (strings).\n",
    "        word_to_index: Dictionary mapping words to their corresponding index\n",
    "                       in a one-hot-encoded representation of our corpus.\n",
    "\n",
    "        window_size:   Size of Skipgram window.\n",
    "                       (use the default value when running your code).\n",
    "\n",
    "    Returns:\n",
    "        (trainX, trainY):     A pair of matrices (trainX, trainY) containing training \n",
    "                              points (one-hot-encoded vectors representing individual words) and \n",
    "                              their corresponding labels (also one-hot-encoded vectors representing words).\n",
    "\n",
    "                              For each index i, trainX[i] should correspond to a word in\n",
    "                              <word_list>, and trainY[i] should correspond to one of the words within\n",
    "                              a window of size <window_size> of trainX[i].\n",
    "    \"\"\"\n",
    "    if char_based:\n",
    "        one_hot = generate_onehot_char_dict(word_list, padding = 0)\n",
    "    else:\n",
    "        one_hot = generate_onehot_word_dict(word_list)\n",
    "        one_hot_forward = one_hot\n",
    "    \n",
    "    trainX = []\n",
    "    trainY = []\n",
    "\n",
    "    for index in range(len(word_list)):\n",
    "        \n",
    " \n",
    "        word = word_list[index]\n",
    "        matrix = one_hot_forward[word]\n",
    "        \n",
    "        for i in [x for x in range(-window_size, window_size + 1) if x != 0]:\n",
    "\n",
    "            if (index + i) >= 0 and (index + i) < len(word_list):\n",
    "\n",
    "                onehot_x = matrix\n",
    "                onehot_y = one_hot_forward[word_list[index + i]]\n",
    "\n",
    "                trainX.append(onehot_x)\n",
    "                trainY.append(onehot_y)\n",
    "\n",
    "    return (np.array(trainX), np.array(trainY)), one_hot_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding(word_list, num_latent_factors=10, char_based = False):\n",
    "    \"\"\" \n",
    "    Train a word2vec type embedding of the training data.\n",
    "        Data innput format: List\n",
    "        \n",
    "    Uses a dense, two layer shallow neural net to embed.  \n",
    "    Arguments:\n",
    "        word_list, constitutes training data\n",
    "        num_latent_factors, number of latent factors\n",
    "        \n",
    "    \"\"\"\n",
    "    train_data = generate_traindata(word_list, char_based = False)\n",
    "\n",
    "    trainX, trainY = train_data[0][0], train_data[0][1]\n",
    "\n",
    "    one_hot_dict = train_data[1]\n",
    "    \n",
    "\n",
    "#     input_dimension = 26\n",
    "    input_dimension = 3176\n",
    "    input_shape = (len(trainX[0]), 26)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "#     model.add(Dense(num_latent_factors, input_shape=input_shape))\n",
    "#     model.add(Dense(21))\n",
    "#     model.add(Activation(\"softmax\"))   \n",
    "\n",
    "#     model.add(Dense(input_dimension, input_shape=input_shape)), activation=\"relu\"))\n",
    "#     model.add(Dense(num_latent_factors)), activation=\"relu\"))\n",
    "#     model.add(Dense(input_dimension))\n",
    "#     model.add(Activation(\"softmax\"))\n",
    "\n",
    "    model.add(Dense(num_latent_factors, input_dim=input_dimension))\n",
    "    model.add(Dense(3176))\n",
    "    model.add(Activation(\"softmax\"))   \n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "    print(\"\\n\")\n",
    "\n",
    "    model.fit(trainX, trainY, batch_size=30, epochs=5)\n",
    "\n",
    "    # Get model weights up to the second layer\n",
    "    weights_layer_1 = model.get_weights()[0]\n",
    "    weights_layer_2 = model.get_weights()[1]\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    for weight in model.get_weights():\n",
    "        print(\"Weight Shape:\", weight.shape)\n",
    "\n",
    "    # Sanity check, predict words from 30 random words in the training set\n",
    "    #random_words = np.random.choice(trainX,30)\n",
    "    model_output = model.predict(trainX[:50])\n",
    "    \n",
    "    if char_based:\n",
    "        input_word_list = []\n",
    "        output_word_list = []\n",
    "\n",
    "        for i in model_output:\n",
    "            for j in range(len(i)):\n",
    "                index = np.argmax(i[j])\n",
    "                i[j] = np.zeros(len(i[j]))\n",
    "                i[j][index] = 1\n",
    "\n",
    "            output_word_list.append(one_hot_decoder(i, word_list, char_based = True))\n",
    "\n",
    "        for word in random_words:\n",
    "            input_word_list.append(one_hot_decoder(word, word_list, char_based = True))\n",
    "\n",
    "\n",
    "    else:\n",
    "        output_word_list = []\n",
    "        input_word_list = []\n",
    "        \n",
    "        for i in model_output:\n",
    "            index = np.argmax(i)\n",
    "            i = np.zeros(len(i))\n",
    "            i[index] = 1\n",
    "            output_word_list.append(one_hot_decoder(i, word_list))\n",
    "        \n",
    "        for word in trainX[:50]:\n",
    "            input_word_list.append(one_hot_decoder(word, word_list))\n",
    "     \n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"Most likely next word: \")\n",
    "\n",
    "    for k in range(len(input_word_list)):\n",
    "        print(\"\\n\")\n",
    "        print(input_word_list[k], \", \", output_word_list[k])\n",
    "\n",
    "    return weights_layer_1, weights_layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 10)                31770     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3176)              34936     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 3176)              0         \n",
      "=================================================================\n",
      "Total params: 66,706\n",
      "Trainable params: 66,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "\n",
      "Epoch 1/5\n",
      "140636/140636 [==============================] - 17s 123us/step - loss: 6.5409 - accuracy: 0.0273\n",
      "Epoch 2/5\n",
      "140636/140636 [==============================] - 16s 116us/step - loss: 6.4073 - accuracy: 0.0276\n",
      "Epoch 3/5\n",
      "140636/140636 [==============================] - 16s 115us/step - loss: 6.4033 - accuracy: 0.0278\n",
      "Epoch 4/5\n",
      "140636/140636 [==============================] - 16s 115us/step - loss: 6.3995 - accuracy: 0.0278\n",
      "Epoch 5/5\n",
      " 97980/140636 [===================>..........] - ETA: 4s - loss: 6.3881 - accuracy: 0.0285"
     ]
    }
   ],
   "source": [
    "test = train_embedding(word_list, num_latent_factors = 10, char_based = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
